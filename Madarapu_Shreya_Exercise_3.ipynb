{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyamadarapu/INFO_5731/blob/main/Madarapu_Shreya_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "34e8f174-9683-4d21-8ea3-30262614d036"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSentiment analysis of product reviews from e-commerce platforms could be an interesting text classification problem. To automatically categorize each review into good, negative, or neutral sentiment categories is the aim of this work.\\n\\nThe following characteristics could be helpful in developing a sentiment analysis machine learning model:\\n\\n1. Bag of Words(BoW):\\n   - Description: Word occurrence in the text is represented by BoW, which does not take into account the words\\' chronological order of appearance.\\n   - Advantages: BoW records the occurrence or non-occurrence of particular words that convey emotion. Positive emotions, for example, are often connected to terms like \"great,\" \"excellent,\" or \"love,\" whereas negative emotions are often related with terms like \"poor,\" \"disappointing,\" or \"bad.\"\\n\\n2. Inverse Document Frequency-Term Frequency (TF-IDF):\\n   - Description: TF-IDF determines a word\\'s significance within a document based on how frequently it appears in all of the corpus\\'s documents.\\n   - Advantages: By downplaying words that are prevalent in numerous papers, TF-IDF aids in prioritizing terms that are more specific to a certain document. This can assist in locating important terms that convey emotion.\\n\\n3. POS Tags: \\n   - Description: POS tagging is assigning the appropriate part of speech (noun, verb, adjective) to every word in the text.\\n   - Advantages: Sentence structure can reveal important information about sentiment when it is understood. For example, adverbs and adjectives frequently convey polarity in sentiment. \"Unfortunately\" is negative, whereas \"happy\" is good.\\n\\n4. Sentiment Lexicons(Lexical analysis):\\n   - Synopsis: Lists of words that have been carefully chosen and labeled with the appropriate sentiment polarity (positive, negative, or neutral) are called sentiment lexicons.\\n   - Advantages: The model can directly incorporate knowledge about the sentiment of individual phrases by utilizing sentiment lexicons. This can assist in grading texts\\' sentiment according to the percentage of positive or negative terms.\\n\\n5. Word Embeddings: \\n   - Synopsis: Word embeddings depict words as low-dimensional, dense vectors in a continuous vector space, with identical representations for words with similar semantic content.\\n   - Advantages: By capturing the semantic relationships between words, word embeddings enable the model to comprehend the context and meaning of words in addition to their singular occurrences. Word embeddings, which depict words in a continuous vector space, are able to capture sentiment nuances that conventional feature representations, such as Bag of Words, could miss. This aids in the model\\'s improved ability to handle terms that are not in its lexicon and to generalize to new sets of data. Furthermore, by utilizing pre-trained word embeddings such as Word2Vec, GloVe, or fastText, the model can be significantly enhanced in its capacity to identify sentiment in product output by receiving rich semantic information learnt from big text corpora.\\n\\nWith the integration of these varied elements, the machine learning model is able to more accurately classify sentiments by capturing different components of the text, such as the presence of particular words, their grammatical structure, and their contextual usage.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Sentiment analysis of product reviews from e-commerce platforms could be an interesting text classification problem. To automatically categorize each review into good, negative, or neutral sentiment categories is the aim of this work.\n",
        "\n",
        "The following characteristics could be helpful in developing a sentiment analysis machine learning model:\n",
        "\n",
        "1. Bag of Words(BoW):\n",
        "   - Description: Word occurrence in the text is represented by BoW, which does not take into account the words' chronological order of appearance.\n",
        "   - Advantages: BoW records the occurrence or non-occurrence of particular words that convey emotion. Positive emotions, for example, are often connected to terms like \"great,\" \"excellent,\" or \"love,\" whereas negative emotions are often related with terms like \"poor,\" \"disappointing,\" or \"bad.\"\n",
        "\n",
        "2. Inverse Document Frequency-Term Frequency (TF-IDF):\n",
        "   - Description: TF-IDF determines a word's significance within a document based on how frequently it appears in all of the corpus's documents.\n",
        "   - Advantages: By downplaying words that are prevalent in numerous papers, TF-IDF aids in prioritizing terms that are more specific to a certain document. This can assist in locating important terms that convey emotion.\n",
        "\n",
        "3. POS Tags:\n",
        "   - Description: POS tagging is assigning the appropriate part of speech (noun, verb, adjective) to every word in the text.\n",
        "   - Advantages: Sentence structure can reveal important information about sentiment when it is understood. For example, adverbs and adjectives frequently convey polarity in sentiment. \"Unfortunately\" is negative, whereas \"happy\" is good.\n",
        "\n",
        "4. Sentiment Lexicons(Lexical analysis):\n",
        "   - Synopsis: Lists of words that have been carefully chosen and labeled with the appropriate sentiment polarity (positive, negative, or neutral) are called sentiment lexicons.\n",
        "   - Advantages: The model can directly incorporate knowledge about the sentiment of individual phrases by utilizing sentiment lexicons. This can assist in grading texts' sentiment according to the percentage of positive or negative terms.\n",
        "\n",
        "5. Word Embeddings:\n",
        "   - Synopsis: Word embeddings depict words as low-dimensional, dense vectors in a continuous vector space, with identical representations for words with similar semantic content.\n",
        "   - Advantages: By capturing the semantic relationships between words, word embeddings enable the model to comprehend the context and meaning of words in addition to their singular occurrences. Word embeddings, which depict words in a continuous vector space, are able to capture sentiment nuances that conventional feature representations, such as Bag of Words, could miss. This aids in the model's improved ability to handle terms that are not in its lexicon and to generalize to new sets of data. Furthermore, by utilizing pre-trained word embeddings such as Word2Vec, GloVe, or fastText, the model can be significantly enhanced in its capacity to identify sentiment in product output by receiving rich semantic information learnt from big text corpora.\n",
        "\n",
        "With the integration of these varied elements, the machine learning model is able to more accurately classify sentiments by capturing different components of the text, such as the presence of particular words, their grammatical structure, and their contextual usage.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('opinion_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI25RD0aXq7h",
        "outputId": "5037ef5c-d127-4865-ac91-5a2136c08bc7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b81e35-b76a-4e54-d1d6-7bf7f8caf708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Bag of Words (BoW):\n",
            "['bad' 'did' 'disappointed' 'excellent' 'expectations' 'experience'\n",
            " 'great' 'had' 'highly' 'is' 'it' 'love' 'meet' 'my' 'not' 'of' 'poor'\n",
            " 'product' 'quality' 'recommended' 'the' 'this' 'unfortunately' 'with']\n",
            "[[0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0]\n",
            " [1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1]\n",
            " [0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0]\n",
            " [0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0]]\n",
            "\n",
            "2. TF-IDF:\n",
            "['bad' 'did' 'disappointed' 'excellent' 'expectations' 'experience'\n",
            " 'great' 'had' 'highly' 'is' 'it' 'love' 'meet' 'my' 'not' 'of' 'poor'\n",
            " 'product' 'quality' 'recommended' 'the' 'this' 'unfortunately' 'with']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.50619914 0.         0.         0.33900746 0.50619914 0.50619914\n",
            "  0.         0.         0.         0.         0.         0.2412066\n",
            "  0.         0.         0.         0.2412066  0.         0.        ]\n",
            " [0.         0.         0.41160181 0.         0.         0.\n",
            "  0.         0.         0.         0.27565453 0.         0.\n",
            "  0.         0.         0.         0.41160181 0.41160181 0.19613047\n",
            "  0.41160181 0.         0.41160181 0.19613047 0.         0.        ]\n",
            " [0.47382645 0.         0.         0.         0.         0.47382645\n",
            "  0.         0.47382645 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.22578084\n",
            "  0.         0.         0.         0.22578084 0.         0.47382645]\n",
            " [0.         0.         0.         0.50619914 0.         0.\n",
            "  0.         0.         0.50619914 0.33900746 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.2412066\n",
            "  0.         0.50619914 0.         0.2412066  0.         0.        ]\n",
            " [0.         0.39362408 0.         0.         0.39362408 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.39362408 0.39362408 0.39362408 0.         0.         0.18756398\n",
            "  0.         0.         0.         0.18756398 0.39362408 0.        ]]\n",
            "\n",
            "3. N-grams (Bigrams):\n",
            "[('this', 'product'), ('product', 'is'), ('is', 'great'), ('great', '!'), ('!', 'i'), ('i', 'love'), ('love', 'it'), ('it', '.'), ('the', 'quality'), ('quality', 'of'), ('of', 'this'), ('this', 'product'), ('product', 'is'), ('is', 'poor'), ('poor', '.'), ('.', 'i'), ('i', \"'m\"), (\"'m\", 'disappointed'), ('disappointed', '.'), ('i', 'had'), ('had', 'a'), ('a', 'bad'), ('bad', 'experience'), ('experience', 'with'), ('with', 'this'), ('this', 'product'), ('product', '.'), ('this', 'product'), ('product', 'is'), ('is', 'excellent'), ('excellent', '.'), ('.', 'highly'), ('highly', 'recommended'), ('recommended', '.'), ('unfortunately', ','), (',', 'this'), ('this', 'product'), ('product', 'did'), ('did', 'not'), ('not', 'meet'), ('meet', 'my'), ('my', 'expectations'), ('expectations', '.')]\n",
            "\n",
            "4. POS Tags:\n",
            "[('this', 'DT'), ('product', 'NN'), ('is', 'VBZ'), ('great', 'JJ'), ('!', '.'), ('i', 'NN'), ('love', 'VBP'), ('it', 'PRP'), ('.', '.')]\n",
            "[('the', 'DT'), ('quality', 'NN'), ('of', 'IN'), ('this', 'DT'), ('product', 'NN'), ('is', 'VBZ'), ('poor', 'JJ'), ('.', '.'), ('i', 'JJ'), (\"'m\", 'VBP'), ('disappointed', 'JJ'), ('.', '.')]\n",
            "[('i', 'NN'), ('had', 'VBD'), ('a', 'DT'), ('bad', 'JJ'), ('experience', 'NN'), ('with', 'IN'), ('this', 'DT'), ('product', 'NN'), ('.', '.')]\n",
            "[('this', 'DT'), ('product', 'NN'), ('is', 'VBZ'), ('excellent', 'JJ'), ('.', '.'), ('highly', 'RB'), ('recommended', 'JJ'), ('.', '.')]\n",
            "[('unfortunately', 'RB'), (',', ','), ('this', 'DT'), ('product', 'NN'), ('did', 'VBD'), ('not', 'RB'), ('meet', 'VB'), ('my', 'PRP$'), ('expectations', 'NNS'), ('.', '.')]\n",
            "\n",
            "5. Sentiment Scores:\n",
            "[0.2222222222222222, -0.16666666666666666, -0.1111111111111111, 0.25, -0.1]\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\"\"\" As the main intention of this program was to demonstrate feauture extraction,\n",
        "I have just collected only 5 sample reviews of a beauty product randomly taken from amazon so that it can be easy for analysis.\n",
        "\"\"\"\n",
        "#Sample text data\n",
        "texts = [\n",
        "    \"This product is great! I love it.\",\n",
        "    \"The quality of this product is poor. I'm disappointed.\",\n",
        "    \"I had a bad experience with this product.\",\n",
        "    \"This product is excellent. Highly recommended.\",\n",
        "    \"Unfortunately, this product did not meet my expectations.\"\n",
        "]\n",
        "\n",
        "# Bag of Words (BoW)\n",
        "def get_bow(texts):\n",
        "    # Instantiate a CountVectorizer object, which is used to convert a collection of text documents into a matrix of token counts\n",
        "    vectorizer = CountVectorizer()\n",
        "    # Fit and transform the input texts using the CountVectorizer.\n",
        "    # This step tokenizes the texts, builds the vocabulary, and transforms the texts into a sparse matrix of word counts.\n",
        "    bow_matrix = vectorizer.fit_transform(texts)\n",
        "    # Convert the sparse matrix of word counts to a dense array representation.\n",
        "    # This is done using the toarray() method of the sparse matrix.\n",
        "    # The resulting array represents the Bag of Words (BoW) representation of the input texts.\n",
        "    # Each row corresponds to a document in the input texts, and each column corresponds to a unique word in the vocabulary.\n",
        "    # The value at each position [i, j] in the array represents the count of the j-th word in the i-th document.\n",
        "    # Retrieve the feature names from the CountVectorizer object.\n",
        "    # These feature names correspond to the unique words in the vocabulary, in the order they appear in the columns of the BoW array.\n",
        "    # The get_feature_names_out() method returns the feature names.\n",
        "    return bow_matrix.toarray(), vectorizer.get_feature_names_out()\n",
        "\n",
        "# TF-IDF\n",
        "def get_tfidf(texts):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # Transform the input texts into TF-IDF matrix\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    # Convert the TF-IDF matrix into a dense array representation\n",
        "    # toarray() method converts the sparse matrix to a dense matrix\n",
        "    # Retrieve the feature names from the vectorizer\n",
        "    # get_feature_names_out() returns the feature names from the vectorizer\n",
        "    return tfidf_matrix.toarray(), vectorizer.get_feature_names_out()\n",
        "\n",
        "# N-grams\n",
        "def get_ngrams(texts, n=2):\n",
        "    ngrams_list = []\n",
        "    for text in texts:\n",
        "        # Tokenize the current text into words and convert them to lowercase\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        # Generate all possible n-grams from the tokenized words and convert the result into a list\n",
        "        # Then, extend the ngrams_list with the generated n-grams for the current text\n",
        "        ngrams_list.extend(list(ngrams(tokens, n)))\n",
        "    return ngrams_list\n",
        "\n",
        "# POS Tags\n",
        "def get_pos_tags(texts):\n",
        "    pos_tags_list = []\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        # Use the NLTK's pos_tag function to assign POS tags to each token in the text\n",
        "        # and append the tagged tokens to the pos_tags_list\n",
        "        pos_tags_list.append(pos_tag(tokens))\n",
        "    return pos_tags_list\n",
        "\n",
        "# Sentiment Lexicons\n",
        "def get_sentiment_lexicons(texts):\n",
        "    # Load sentiment lexicons (positive and negative words)\n",
        "    positive_words = set(nltk.corpus.opinion_lexicon.words('positive-words.txt'))\n",
        "    negative_words = set(nltk.corpus.opinion_lexicon.words('negative-words.txt'))\n",
        "\n",
        "    sentiment_scores = []\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        positive_count = sum(1 for token in tokens if token in positive_words)\n",
        "        negative_count = sum(1 for token in tokens if token in negative_words)\n",
        "        total_tokens = len(tokens)\n",
        "        # Calculating the sentiment score for the text based on the difference between positive and negative word counts\n",
        "        # If there are no tokens in the text (empty text), assign a sentiment score of 0 to avoid division by zero\n",
        "        sentiment_score = (positive_count - negative_count) / total_tokens if total_tokens != 0 else 0\n",
        "        sentiment_scores.append(sentiment_score)\n",
        "    return sentiment_scores\n",
        "\n",
        "bow_matrix, bow_features = get_bow(texts)\n",
        "tfidf_matrix, tfidf_features = get_tfidf(texts)\n",
        "bigrams = get_ngrams(texts, n=2)\n",
        "pos_tags = get_pos_tags(texts)\n",
        "sentiment_scores = get_sentiment_lexicons(texts)\n",
        "\n",
        "print(\"1. Bag of Words (BoW):\")\n",
        "print(bow_features)\n",
        "print(bow_matrix)\n",
        "\n",
        "print(\"\\n2. TF-IDF:\")\n",
        "print(tfidf_features)\n",
        "print(tfidf_matrix)\n",
        "\n",
        "print(\"\\n3. N-grams (Bigrams):\")\n",
        "print(bigrams)\n",
        "\n",
        "print(\"\\n4. POS Tags:\")\n",
        "for tags in pos_tags:\n",
        "    print(tags)\n",
        "\n",
        "print(\"\\n5. Sentiment Scores:\")\n",
        "print(sentiment_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5296fc60-2883-4cd6-b947-ef6e1fb68b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Features based on Document Frequency (DF):\n",
            "Rank 1: product (Document Frequency: 5)\n",
            "Rank 2: this (Document Frequency: 5)\n",
            "Rank 3: is (Document Frequency: 3)\n",
            "Rank 4: bad (Document Frequency: 1)\n",
            "Rank 5: did (Document Frequency: 1)\n",
            "Rank 6: disappointed (Document Frequency: 1)\n",
            "Rank 7: excellent (Document Frequency: 1)\n",
            "Rank 8: expectations (Document Frequency: 1)\n",
            "Rank 9: experience (Document Frequency: 1)\n",
            "Rank 10: great (Document Frequency: 1)\n",
            "Rank 11: had (Document Frequency: 1)\n",
            "Rank 12: highly (Document Frequency: 1)\n",
            "Rank 13: it (Document Frequency: 1)\n",
            "Rank 14: love (Document Frequency: 1)\n",
            "Rank 15: meet (Document Frequency: 1)\n",
            "Rank 16: my (Document Frequency: 1)\n",
            "Rank 17: not (Document Frequency: 1)\n",
            "Rank 18: of (Document Frequency: 1)\n",
            "Rank 19: poor (Document Frequency: 1)\n",
            "Rank 20: quality (Document Frequency: 1)\n",
            "Rank 21: recommended (Document Frequency: 1)\n",
            "Rank 22: the (Document Frequency: 1)\n",
            "Rank 23: unfortunately (Document Frequency: 1)\n",
            "Rank 24: with (Document Frequency: 1)\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\"\"\"\n",
        "    The 4 feature selection methods used for text classification in the given paper are:\n",
        "    - Filter model\n",
        "    - Wrapper model\n",
        "    - Embedded model and\n",
        "    - Hybrid model\n",
        "\n",
        "    From the above am using Document Frequency method from Filter model which makes simpler for ranking\n",
        "    And i am choosing Bag of words(BOW) for ranking purpose\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"This product is great! I love it.\",\n",
        "    \"The quality of this product is poor. I'm disappointed.\",\n",
        "    \"I had a bad experience with this product.\",\n",
        "    \"This product is excellent. Highly recommended.\",\n",
        "    \"Unfortunately, this product did not meet my expectations.\"\n",
        "]\n",
        "\n",
        "# Convert text data to lowercase for consistency\n",
        "texts_lower = [text.lower() for text in texts]\n",
        "\n",
        "# Initialize CountVectorizer to create Bag of Words (BoW) representation\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the text data to BoW matrix\n",
        "bow_matrix = vectorizer.fit_transform(texts_lower)\n",
        "\n",
        "# Get feature names (terms) from CountVectorizer\n",
        "bow_features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Calculate document frequency (DF) for each term in BoW features\n",
        "document_frequency = bow_matrix.sum(axis=0)\n",
        "\n",
        "# Convert document frequency to a list\n",
        "document_frequency_list = document_frequency.tolist()[0]\n",
        "\n",
        "# Combine features with their corresponding document frequency\n",
        "features_df = list(zip(bow_features, document_frequency_list))\n",
        "\n",
        "# Rank features based on document frequency in descending order\n",
        "ranked_features_df = sorted(features_df, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print ranked features based on document frequency\n",
        "print(\"Ranked Features based on Document Frequency (DF):\")\n",
        "for rank, (feature, df) in enumerate(ranked_features_df, start=1):\n",
        "    print(f\"Rank {rank}: {feature} (Document Frequency: {df})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3jxtvM4ax51",
        "outputId": "fea8a8dc-6925-4b55-dc97-b104e8002067"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61a2871-dc58-47af-ee1f-27e75175a4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Texts based on Similarity with Query:\n",
            "Rank 1: This product is excellent. Highly recommended. (Similarity: 0.8092)\n",
            "Rank 2: This product is great! I love it. (Similarity: 0.7602)\n",
            "Rank 3: Unfortunately, this product did not meet my expectations. (Similarity: 0.4228)\n",
            "Rank 4: I had a bad experience with this product. (Similarity: 0.3954)\n",
            "Rank 5: The quality of this product is poor. I'm disappointed. (Similarity: 0.3765)\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample data\n",
        "texts = [\n",
        "    \"This product is great! I love it.\",\n",
        "    \"The quality of this product is poor. I'm disappointed.\",\n",
        "    \"I had a bad experience with this product.\",\n",
        "    \"This product is excellent. Highly recommended.\",\n",
        "    \"Unfortunately, this product did not meet my expectations.\"\n",
        "]\n",
        "\n",
        "# Query\n",
        "query = \"I'm looking for a high-quality product.\"\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Encode query and texts into BERT embeddings\n",
        "query_embedding = model.encode(query)\n",
        "text_embeddings = model.encode(texts)\n",
        "\n",
        "# Calculate cosine similarity between query and each text\n",
        "# Encode both the query and the texts into BERT embeddings using the loaded model.\n",
        "similarities = cosine_similarity([query_embedding], text_embeddings)[0]\n",
        "\n",
        "# Combining texts with their similarity scores\n",
        "text_similarities = list(zip(texts, similarities))\n",
        "\n",
        "# Ranking texts based on similarity scores in descending order\n",
        "ranked_texts = sorted(text_similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the ranked texts\n",
        "print(\"Ranked Texts based on Similarity with Query:\")\n",
        "for rank, (text, similarity) in enumerate(ranked_texts, start=1):\n",
        "    print(f\"Rank {rank}: {text} (Similarity: {similarity:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "My experience with this assignment has confirmed how crucial it is to preprocess text data in order to guarantee consistency and boost the efficiency of feature extraction techniques.\n",
        "Gaining an understanding of several feature extraction methods, including word embeddings, TF-IDF, N-grams, and Bag of Words (BoW), made it possible to thoroughly explore the representation of text data. Acquiring knowledge of feature selection techniques like Chi-Square and Document Frequency also helped with feature set optimization for text classification applications.\n",
        "\n",
        "The main ideas and methods that I thought were most helpful are as follows:\n",
        "1. Preprocessing methods to get text data ready for feature extraction, such as tokenization, lowercasing, and stopword removal.\n",
        "2. Recognizing how feature extraction techniques such as BoW, TF-IDF, and N-grams capture various facets of textual data and the understanding underlying them.\n",
        "3. Examining word embeddings and how well they can convey word semantic links.\n",
        "4. Acquiring knowledge about feature selection techniques and how choosing the most pertinent features can lower dimensionality and enhance model performance.\n",
        "\n",
        "Making sure external libraries like the Sentence Transformers library for sentence embeddings and the Hugging Face Transformers library for BERT embeddings installed and were compatible was one of the challenges faced.\n",
        "It was essential to resolve dependencies and make sure these libraries were installed correctly in order to apply sophisticated feature extraction methods.\n",
        "\n",
        "This practice has a lot to do with natural language processing (NLP). An essential first step in natural language processing (NLP) activities like information retrieval, text classification, and sentiment analysis is feature extraction. The performance of NLP models can be greatly impacted by choosing the right features and being aware of various feature extraction strategies. Furthermore, in NLP applications, feature selection techniques are essential for increasing model interpretability, decreasing overfitting, and raising overall efficiency. As a result, it is crucial for NLP practitioners to become proficient in feature extraction and selection approaches.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "8db9dd20-15df-4133-e381-dc3d5bea7a5b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMy experience with this assignment has confirmed how crucial it is to preprocess text data in order to guarantee consistency and boost the efficiency of feature extraction techniques. \\nGaining an understanding of several feature extraction methods, including word embeddings, TF-IDF, N-grams, and Bag of Words (BoW), made it possible to thoroughly explore the representation of text data. Acquiring knowledge of feature selection techniques like Chi-Square and Document Frequency also helped with feature set optimization for text classification applications.\\n\\nThe main ideas and methods that I thought were most helpful are as follows: \\n1. Preprocessing methods to get text data ready for feature extraction, such as tokenization, lowercasing, and stopword removal.\\n2. Recognizing how feature extraction techniques such as BoW, TF-IDF, and N-grams capture various facets of textual data and the understanding underlying them.\\n3. Examining word embeddings and how well they can convey word semantic links.\\n4. Acquiring knowledge about feature selection techniques and how choosing the most pertinent features can lower dimensionality and enhance model performance.\\n\\nMaking sure external libraries like the Sentence Transformers library for sentence embeddings and the Hugging Face Transformers library for BERT embeddings installed and were compatible was one of the challenges faced. \\nIt was essential to resolve dependencies and make sure these libraries were installed correctly in order to apply sophisticated feature extraction methods.\\n\\nThis practice has a lot to do with natural language processing (NLP). An essential first step in natural language processing (NLP) activities like information retrieval, text classification, and sentiment analysis is feature extraction. The performance of NLP models can be greatly impacted by choosing the right features and being aware of various feature extraction strategies. Furthermore, in NLP applications, feature selection techniques are essential for increasing model interpretability, decreasing overfitting, and raising overall efficiency. As a result, it is crucial for NLP practitioners to become proficient in feature extraction and selection approaches.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}